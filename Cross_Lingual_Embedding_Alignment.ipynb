{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOjEITN/ev8/IlXDnJ2xEwE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praj-pawar/Aligning-Cross-Lingual-Embeddings/blob/main/Cross_Lingual_Embedding_Alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1) Data Preparation"
      ],
      "metadata": {
        "id": "lmoyUnpZKmma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1a - Download Pre-Trained FastText Word Embeddings"
      ],
      "metadata": {
        "id": "eHSZQFxeAE8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz  # English\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz  # Hindi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkTFQP9X-IU5",
        "outputId": "0df1542b-39ae-4b91-f79b-2d35777cf8f0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-18 17:44:57--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.15, 13.226.210.25, 13.226.210.111, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G  82.6MB/s    in 12s     \n",
            "\n",
            "2024-09-18 17:45:09 (106 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n",
            "--2024-09-18 17:45:09--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.226.210.15, 13.226.210.25, 13.226.210.111, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.226.210.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1118942272 (1.0G) [binary/octet-stream]\n",
            "Saving to: ‘cc.hi.300.vec.gz’\n",
            "\n",
            "cc.hi.300.vec.gz    100%[===================>]   1.04G   161MB/s    in 11s     \n",
            "\n",
            "2024-09-18 17:45:20 (96.3 MB/s) - ‘cc.hi.300.vec.gz’ saved [1118942272/1118942272]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1b - Load the Embeddings"
      ],
      "metadata": {
        "id": "0FOtiMitAXC_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQFF13y09xgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "4a479da7-bd5c-4939-fff9-b39f73b1508b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100000 English embeddings\n",
            "Loaded 100000 Hindi embeddings\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "def load_fasttext_embeddings(file_path, top_n=100000):\n",
        "    embeddings = {}\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i == 0:  # Skip the header line in FastText files\n",
        "                continue\n",
        "            if i > top_n:\n",
        "                break\n",
        "            tokens = line.decode('utf-8').strip().split(' ')\n",
        "            word = tokens[0]\n",
        "            vector = np.array(tokens[1:], dtype=np.float32)\n",
        "            vector = vector / np.linalg.norm(vector)\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Load English and Hindi embeddings (Top 100,000 words)\n",
        "# Sorted by decreasing order of frequency by default (FastText rocks)\n",
        "en_embeddings = load_fasttext_embeddings('cc.en.300.vec.gz', top_n=100000)\n",
        "hi_embeddings = load_fasttext_embeddings('cc.hi.300.vec.gz', top_n=100000)\n",
        "\n",
        "print(f\"Loaded {len(en_embeddings)} English embeddings\")\n",
        "print(f\"Loaded {len(hi_embeddings)} Hindi embeddings\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1c - Download the English-Hindi dictionary from MUSE\n"
      ],
      "metadata": {
        "id": "0TBkmAvzAbyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "er7DPSxbgP4c",
        "outputId": "b553c937-2199-4158-a391-83624b8c4114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-18 17:46:25--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.155.173.116, 18.155.173.80, 18.155.173.40, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.155.173.116|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930856 (909K) [text/x-c++]\n",
            "Saving to: ‘en-hi.txt’\n",
            "\n",
            "en-hi.txt           100%[===================>] 909.04K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-09-18 17:46:25 (7.84 MB/s) - ‘en-hi.txt’ saved [930856/930856]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1c - Display Pairs"
      ],
      "metadata": {
        "id": "UT-WD8SmAvkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_bilingual_lexicon(file_path):\n",
        "    bilingual_dict = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            en_word, hi_word = line.strip().split()\n",
        "            bilingual_dict.append((en_word, hi_word))\n",
        "    return bilingual_dict\n",
        "\n",
        "# Load English-Hindi word pairs\n",
        "en_hi_pairs = load_bilingual_lexicon('en-hi.txt')\n",
        "\n",
        "print(en_hi_pairs[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snp8ndm5gZEa",
        "outputId": "7418bb04-6fd4-4e3d-f2a5-9d3e2fc5e9a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('and', 'और'), ('was', 'था'), ('was', 'थी'), ('for', 'लिये'), ('that', 'उस'), ('that', 'कि'), ('with', 'साथ'), ('from', 'से'), ('from', 'इससे'), ('this', 'ये')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1c - Extract Word Embeddings for Bilingual Word Pairs"
      ],
      "metadata": {
        "id": "wCujQTNRLHFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to extract word embeddings for the bilingual word pairs\n",
        "def extract_word_embeddings(bilingual_pairs, en_embeddings, hi_embeddings):\n",
        "    en_vecs = []\n",
        "    hi_vecs = []\n",
        "\n",
        "    for en_word, hi_word in bilingual_pairs:\n",
        "        if en_word in en_embeddings and hi_word in hi_embeddings:\n",
        "            en_vecs.append(en_embeddings[en_word])\n",
        "            hi_vecs.append(hi_embeddings[hi_word])\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    en_vecs = np.array(en_vecs)\n",
        "    hi_vecs = np.array(hi_vecs)\n",
        "\n",
        "    return en_vecs, hi_vecs\n",
        "\n",
        "# Extract English and Hindi embeddings for the bilingual lexicon\n",
        "en_vecs, hi_vecs = extract_word_embeddings(en_hi_pairs, en_embeddings, hi_embeddings)\n",
        "\n",
        "print(f\"Extracted {en_vecs.shape[0]} aligned word vectors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei3DaO3mgnX5",
        "outputId": "e6911149-bda5-4f01-d0b2-3f42dd656421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 18972 aligned word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) Embedding Alignment"
      ],
      "metadata": {
        "id": "fZ8nhE-4LXNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2a & 2b - Implement Orthogonal Procrustes Alignment"
      ],
      "metadata": {
        "id": "DzIWQJeGLayB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def orthogonal_procrustes(X, Y):\n",
        "    \"\"\"\n",
        "    Perform orthogonal Procrustes alignment to learn a mapping from X to Y.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy array): Source language word embeddings (English)\n",
        "    Y (numpy array): Target language word embeddings (Hindi)\n",
        "\n",
        "    Returns:\n",
        "    W (numpy array): Orthogonal transformation matrix\n",
        "    \"\"\"\n",
        "    X = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
        "    Y = Y / np.linalg.norm(Y, axis=1, keepdims=True)\n",
        "    # Compute matrix product of X^T and Y\n",
        "    M = np.dot(X.T, Y)\n",
        "\n",
        "    # Perform SVD on the matrix M\n",
        "    U, _, Vt = np.linalg.svd(M)\n",
        "\n",
        "    # Compute the orthogonal transformation matrix W\n",
        "    W = np.dot(U, Vt)\n",
        "\n",
        "    return W\n",
        "\n",
        "W = orthogonal_procrustes(en_vecs, hi_vecs)\n",
        "\n",
        "print(\"Orthogonal mapping matrix learned.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmA3RxaSg3-C",
        "outputId": "e6a462ca-f75f-41a2-8c82-ab9f04e6b499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orthogonal mapping matrix learned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_mapping(embeddings, W):\n",
        "    \"\"\"\n",
        "    Apply the learned orthogonal mapping to the source language embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    embeddings (dict): Source language embeddings (English)\n",
        "    W (numpy array): Orthogonal transformation matrix\n",
        "\n",
        "    Returns:\n",
        "    mapped_embeddings (dict): Transformed embeddings\n",
        "    \"\"\"\n",
        "    mapped_embeddings = {}\n",
        "    for word, vec in embeddings.items():\n",
        "        mapped_vec = np.dot(vec, W)\n",
        "        # Normalize the mapped vector\n",
        "        mapped_vec = mapped_vec / np.linalg.norm(mapped_vec)\n",
        "        mapped_embeddings[word] = mapped_vec\n",
        "    return mapped_embeddings\n",
        "\n",
        "aligned_en_embeddings = apply_mapping(en_embeddings, W)\n",
        "\n",
        "print(f\"Aligned {len(aligned_en_embeddings)} English embeddings into the Hindi space.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjogY41Vg-CC",
        "outputId": "3f8854f9-1fb4-4991-aa48-d866e067a3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aligned 100000 English embeddings into the Hindi space.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3) Evaluation\n"
      ],
      "metadata": {
        "id": "Lr2QCywEBMUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3a - Perform Word Translation\n",
        "###Execution lasted for ~2 hours before the session terminated :(\n",
        "###Hence, limited size of en_words to be translated to 2000."
      ],
      "metadata": {
        "id": "TYCN7qp9-AbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def translate_words(aligned_en_embeddings, hi_embeddings, top_k=5, limit_size=None):\n",
        "    \"\"\"\n",
        "    Translate a limited number of words from English to Hindi using aligned embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    aligned_en_embeddings (dict): Aligned English embeddings (after Procrustes alignment)\n",
        "    hi_embeddings (dict): Hindi embeddings\n",
        "    top_k (int): Number of top nearest neighbors to return\n",
        "    limit_size (int, optional): Limit the number of words to translate\n",
        "\n",
        "    Returns:\n",
        "    translations (dict): Dictionary with English words as keys and top_k Hindi translations as values\n",
        "    \"\"\"\n",
        "    translations = {}\n",
        "    hi_words = list(hi_embeddings.keys())\n",
        "    hi_vecs = np.array(list(hi_embeddings.values()))\n",
        "\n",
        "    # Limit the number of words to translate if limit_size is provided\n",
        "    en_words = list(aligned_en_embeddings.keys())\n",
        "    if limit_size is not None:\n",
        "        en_words = en_words[:limit_size]\n",
        "\n",
        "    for en_word in en_words:\n",
        "        en_vec = aligned_en_embeddings[en_word]\n",
        "        # Compute cosine similarity between the English word vector and all Hindi word vectors\n",
        "        en_vec = en_vec / np.linalg.norm(en_vec)\n",
        "        hi_vecs_norm = hi_vecs / np.linalg.norm(hi_vecs, axis=1, keepdims=True)\n",
        "        similarities = cosine_similarity([en_vec], hi_vecs_norm).flatten()\n",
        "\n",
        "        # Get top_k most similar Hindi words\n",
        "        nearest_idxs = similarities.argsort()[-top_k:][::-1]\n",
        "        nearest_words = [hi_words[i] for i in nearest_idxs]\n",
        "\n",
        "        translations[en_word] = nearest_words\n",
        "\n",
        "    return translations\n",
        "\n",
        "limit_size = 2000  # Set the limit size as needed\n",
        "translations = translate_words(aligned_en_embeddings, hi_embeddings, top_k=5, limit_size=limit_size)\n",
        "\n",
        "# Print a few translations\n",
        "for en_word, hi_words in list(translations.items())[:10]:\n",
        "    print(f\"English: {en_word} -> Hindi: {hi_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-Ar-Z1VhRTa",
        "outputId": "c656a644-ea2b-4637-8ca6-e859125ea36b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: , -> Hindi: ['और', 'वे', '?', ',', 'था']\n",
            "English: the -> Hindi: ['में', 'जिस', 'अपने', 'पहले', 'उसी']\n",
            "English: . -> Hindi: ['¤', 'la', '…', '?', 'हर']\n",
            "English: and -> Hindi: ['साथ', 'तथा', 'करती', 'लिए', 'करके']\n",
            "English: to -> Hindi: ['करके', 'करें', 'करते', 'करना', 'करने']\n",
            "English: of -> Hindi: ['में', 'आने', 'सबसे', 'जिसके', 'जाने']\n",
            "English: a -> Hindi: ['ऐसा', 'बना', 'बड़ा', 'नया', 'अपना']\n",
            "English: </s> -> Hindi: ['.', 'ik', '📝', 'QF', '▲']\n",
            "English: in -> Hindi: ['में', 'सामने', 'क्षेत्र', 'बाहर', 'जहाँ']\n",
            "English: is -> Hindi: ['है', 'यह', 'होता', 'करता', 'माना']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3a - Optimization Attempts\n",
        "\n",
        "1.   cKDTree from scipy.spatial - builds a spatial index of Hindi word vectors. This enables efficient nearest-neighbor search operations.\n",
        "\n",
        "2.   query method of cKDTree allows for querying the k-nearest neighbors efficiently. This avoids computing cosine similarities manually and can be faster, especially for large datasets. (Batch processing).\n",
        "\n",
        "3. Limited Size as the above two standalone changes were\n",
        "also taking more time than expected.\n",
        "\n",
        "##Though faster, massive drop in accuracy noticed.\n",
        "###(0.3722 for size=1000)\n",
        "###(0.35 for size=2000)\n",
        "\n"
      ],
      "metadata": {
        "id": "GNxFWH9N-U5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# import numpy as np\n",
        "# from scipy.spatial import cKDTree\n",
        "\n",
        "# def translate_words_limited(aligned_en_embeddings, hi_embeddings, top_k=5, limit_size=None):\n",
        "#     \"\"\"\n",
        "#     Translate a limited number of words from English to Hindi using aligned embeddings.\n",
        "\n",
        "#     Parameters:\n",
        "#     aligned_en_embeddings (dict): Aligned English embeddings (after Procrustes alignment)\n",
        "#     hi_embeddings (dict): Hindi embeddings\n",
        "#     top_k (int): Number of top nearest neighbors to return\n",
        "#     limit_size (int, optional): Limit the number of words to translate\n",
        "\n",
        "#     Returns:\n",
        "#     translations (dict): Dictionary with English words as keys and top_k Hindi translations as values\n",
        "#     \"\"\"\n",
        "#     translations = {}\n",
        "#     hi_words = list(hi_embeddings.keys())\n",
        "#     hi_vecs = np.array(list(hi_embeddings.values()))\n",
        "\n",
        "#     # Build a KD-tree for the Hindi embeddings\n",
        "#     hi_tree = cKDTree(hi_vecs)\n",
        "\n",
        "#     # Limit the number of words to translate else takes up too much time\n",
        "#     en_words = list(aligned_en_embeddings.keys())\n",
        "#     if limit_size:\n",
        "#         en_words = en_words[:limit_size]\n",
        "\n",
        "#     for en_word in en_words:\n",
        "#         en_vec = aligned_en_embeddings[en_word]\n",
        "#         # Query the KD-tree to find the top_k nearest neighbors\n",
        "#         distances, indices = hi_tree.query([en_vec], k=top_k)\n",
        "\n",
        "#         # Get the nearest words based on indices\n",
        "#         nearest_words = [hi_words[i] for i in indices[0]]\n",
        "\n",
        "#         translations[en_word] = nearest_words\n",
        "\n",
        "#     return translations\n",
        "\n",
        "\n",
        "# limit_size = 2000\n",
        "# translations = translate_words_limited(aligned_en_embeddings, hi_embeddings, top_k=5, limit_size=limit_size)\n",
        "\n",
        "# # Print a few translations\n",
        "# for en_word, hi_words in list(translations.items())[:10]:\n",
        "#     print(f\"English: {en_word} -> Hindi: {hi_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHXdcTxXh9TR",
        "outputId": "edecad04-01bc-45cf-f4b3-88c85d4146f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: , -> Hindi: ['और', ',', 'हैं', 'जहाँ', 'जबकि']\n",
            "English: the -> Hindi: ['में', 'अपने', 'पहले', 'किसी', 'दूसरे']\n",
            "English: . -> Hindi: ['across', 'इतना', 'उनका', 'desde', 'दनकौरी41']\n",
            "English: and -> Hindi: ['दोनों', 'लेकिन', 'जिससे', 'जैसे', 'तथा']\n",
            "English: to -> Hindi: ['करें', 'करके', 'सकें', 'करना', 'करते']\n",
            "English: of -> Hindi: ['में', 'आने', 'सबसे', 'जाने', 'वाले']\n",
            "English: a -> Hindi: ['बना', 'नया', 'ऐसा', 'बड़ा', 'एक']\n",
            "English: </s> -> Hindi: ['.', 'ik', 'nx', 'neq', '50s1']\n",
            "English: in -> Hindi: ['में', 'सामने', 'बाहर', 'उभरकर', 'जहाँ']\n",
            "English: is -> Hindi: ['है', 'यह', 'जो', 'हुआ', 'होता']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3b & 3c - Evaluate P@1, P@5, Translation Accuracy Using the MUSE Test dictionary"
      ],
      "metadata": {
        "id": "Nknr4BZnB2gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_muse_test_dict(file_path):\n",
        "    test_dict = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            words = line.strip().split()\n",
        "            if len(words) == 2:\n",
        "                test_dict[words[0]] = words[1]\n",
        "    return test_dict"
      ],
      "metadata": {
        "id": "rsGRwvITI23S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_translation(translations, test_dict, top_k=5):\n",
        "    \"\"\"\n",
        "    Evaluate translation accuracy using Precision@1, Precision@5, and Accuracy metrics.\n",
        "\n",
        "    Parameters:\n",
        "    translations (dict): Dictionary with English words as keys and top_k Hindi translations as values\n",
        "    test_dict (dict): Dictionary with English words as keys and correct Hindi translations as values\n",
        "    top_k (int): Number of top nearest neighbors considered for Precision@k evaluation\n",
        "\n",
        "    Returns:\n",
        "    precision_at_1 (float): Precision@1 score\n",
        "    precision_at_5 (float): Precision@5 score\n",
        "    accuracy (float): Accuracy score\n",
        "    \"\"\"\n",
        "    true_positives_at_1 = 0\n",
        "    true_positives_at_5 = 0\n",
        "    false_positives_at_1 = 0\n",
        "    false_positives_at_5 = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for en_word, correct_hi_word in test_dict.items():\n",
        "        predicted_hi_words = translations.get(en_word, [])\n",
        "\n",
        "        if len(predicted_hi_words) > 0:\n",
        "            total_predictions += 1\n",
        "\n",
        "            # Precision@1\n",
        "            if correct_hi_word == predicted_hi_words[0]:\n",
        "                true_positives_at_1 += 1\n",
        "                correct_predictions += 1  # Count for accuracy\n",
        "            else:\n",
        "                false_positives_at_1 += 1\n",
        "\n",
        "            # Precision@5\n",
        "            if correct_hi_word in predicted_hi_words[:top_k]:\n",
        "                true_positives_at_5 += 1\n",
        "\n",
        "                # Only count for accuracy if it hasn't been counted for Precision@1\n",
        "                if correct_hi_word != predicted_hi_words[0]:\n",
        "                    correct_predictions += 1\n",
        "            else:\n",
        "                false_positives_at_5 += 1\n",
        "\n",
        "    # Calculate Precision@1\n",
        "    precision_at_1 = true_positives_at_1 / (true_positives_at_1 + false_positives_at_1) if (true_positives_at_1 + false_positives_at_1) > 0 else 0\n",
        "\n",
        "    # Calculate Precision@5\n",
        "    precision_at_5 = true_positives_at_5 / (true_positives_at_5 + false_positives_at_5) if (true_positives_at_5 + false_positives_at_5) > 0 else 0\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "    return precision_at_1, precision_at_5, accuracy\n",
        "\n",
        "test_dict = load_muse_test_dict('en-hi.txt')\n",
        "precision_at_1, precision_at_5, accuracy = evaluate_translation(translations, test_dict)\n",
        "\n",
        "print(f\"Precision@1: {precision_at_1:.4f}\")\n",
        "print(f\"Precision@5: {precision_at_5:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzvmIKRk_qNA",
        "outputId": "d3e65efa-2bc8-4ca0-c647-f070dcc15c4e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@1: 0.3644\n",
            "Precision@5: 0.6547\n",
            "Accuracy: 0.6547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##3d - Cosine Similarity Computation"
      ],
      "metadata": {
        "id": "Sxn8pmgWC0Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_cosine_similarities(en_word_pairs, en_embeddings, hi_embeddings, num_pairs=50):\n",
        "    \"\"\"\n",
        "    Compute cosine similarities between English and Hindi word pairs.\n",
        "\n",
        "    Parameters:\n",
        "    en_word_pairs (list): List of tuples containing (English word, Hindi word) pairs\n",
        "    en_embeddings (dict): Dictionary containing English word embeddings\n",
        "    hi_embeddings (dict): Dictionary containing Hindi word embeddings\n",
        "    num_pairs (int): Number of word pairs to compute similarities for\n",
        "\n",
        "    Returns:\n",
        "    similarities (dict): Dictionary with word pairs as keys and cosine similarity as values\n",
        "    \"\"\"\n",
        "    similarities = {}\n",
        "    count = 0\n",
        "\n",
        "    for en_word, hi_word in en_word_pairs:\n",
        "        if en_word in en_embeddings and hi_word in hi_embeddings:\n",
        "            en_vec = en_embeddings[en_word]\n",
        "            hi_vec = hi_embeddings[hi_word]\n",
        "            en_vec = en_vec / np.linalg.norm(en_vec)\n",
        "            hi_vec = hi_vec / np.linalg.norm(hi_vec)\n",
        "            similarity = cosine_similarity([en_vec], [hi_vec])[0][0]\n",
        "            similarities[(en_word, hi_word)] = similarity\n",
        "\n",
        "            count += 1\n",
        "            if count >= num_pairs:\n",
        "                break\n",
        "\n",
        "    return similarities\n",
        "\n",
        "# Limit the number of word pairs to 50\n",
        "cosine_similarities = compute_cosine_similarities(en_hi_pairs, en_embeddings, hi_embeddings, num_pairs=50)\n",
        "\n",
        "# Print some cosine similarities\n",
        "for (en_word, hi_word), similarity in cosine_similarities.items():\n",
        "    print(f\"English: {en_word}, Hindi: {hi_word}, Similarity: {similarity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mo4k7iAMDovO",
        "outputId": "04e7b3ff-b2ed-4b19-e518-0f70e8198d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: and, Hindi: और, Similarity: 0.0755\n",
            "English: was, Hindi: था, Similarity: -0.0464\n",
            "English: was, Hindi: थी, Similarity: 0.0072\n",
            "English: for, Hindi: लिये, Similarity: -0.0317\n",
            "English: that, Hindi: उस, Similarity: -0.0120\n",
            "English: that, Hindi: कि, Similarity: -0.0811\n",
            "English: with, Hindi: साथ, Similarity: 0.0568\n",
            "English: from, Hindi: से, Similarity: 0.1069\n",
            "English: from, Hindi: इससे, Similarity: 0.0309\n",
            "English: this, Hindi: ये, Similarity: -0.1552\n",
            "English: this, Hindi: यह, Similarity: -0.1453\n",
            "English: this, Hindi: इस, Similarity: -0.2058\n",
            "English: his, Hindi: उसकी, Similarity: 0.0269\n",
            "English: his, Hindi: उसका, Similarity: 0.0216\n",
            "English: his, Hindi: उसके, Similarity: 0.0578\n",
            "English: not, Hindi: नही, Similarity: 0.0132\n",
            "English: not, Hindi: नहीं, Similarity: 0.0316\n",
            "English: are, Hindi: हैं, Similarity: -0.0422\n",
            "English: talk, Hindi: बात, Similarity: -0.0482\n",
            "English: which, Hindi: जिससे, Similarity: 0.0348\n",
            "English: also, Hindi: भी, Similarity: -0.0750\n",
            "English: has, Hindi: रै, Similarity: -0.0903\n",
            "English: were, Hindi: यहूद, Similarity: -0.0206\n",
            "English: but, Hindi: परन्तु, Similarity: 0.0722\n",
            "English: but, Hindi: लेकिन, Similarity: 0.0606\n",
            "English: but, Hindi: लेकीन, Similarity: 0.0225\n",
            "English: but, Hindi: मगर, Similarity: 0.0374\n",
            "English: but, Hindi: लकिन, Similarity: 0.0750\n",
            "English: one, Hindi: एक, Similarity: 0.0040\n",
            "English: new, Hindi: नया, Similarity: 0.1166\n",
            "English: new, Hindi: नई, Similarity: 0.0069\n",
            "English: first, Hindi: प्रथम, Similarity: -0.0359\n",
            "English: first, Hindi: पहली, Similarity: -0.0717\n",
            "English: first, Hindi: पहले, Similarity: -0.0037\n",
            "English: first, Hindi: पहला, Similarity: 0.0480\n",
            "English: page, Hindi: पृष्ठ, Similarity: -0.0693\n",
            "English: page, Hindi: पेज, Similarity: -0.1299\n",
            "English: you, Hindi: आपको, Similarity: 0.1092\n",
            "English: you, Hindi: आप, Similarity: 0.0384\n",
            "English: you, Hindi: तुम, Similarity: 0.0154\n",
            "English: they, Hindi: उन्होंने, Similarity: 0.0693\n",
            "English: they, Hindi: वे, Similarity: 0.0095\n",
            "English: had, Hindi: था, Similarity: -0.0679\n",
            "English: article, Hindi: लेख, Similarity: -0.0244\n",
            "English: article, Hindi: आलेख, Similarity: -0.0179\n",
            "English: who, Hindi: जिसने, Similarity: 0.0361\n",
            "English: who, Hindi: कौन, Similarity: 0.0981\n",
            "English: who, Hindi: जो, Similarity: 0.0258\n",
            "English: all, Hindi: सभी, Similarity: 0.0993\n",
            "English: all, Hindi: सब, Similarity: 0.0341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3e - Ablation Study"
      ],
      "metadata": {
        "id": "xtqtG7vwD8k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_ablation_study(en_embeddings, hi_embeddings, lexicon_sizes=[5000, 10000]):\n",
        "    results = {}\n",
        "    all_pairs = load_bilingual_lexicon('en-hi.txt')\n",
        "\n",
        "    for size in lexicon_sizes:\n",
        "        print(f\"Performing alignment with lexicon size: {size}\")\n",
        "        # Use a subset of the bilingual lexicon\n",
        "        en_hi_pairs = all_pairs[:size]\n",
        "\n",
        "        # Extract embeddings for the word pairs\n",
        "        en_vecs, hi_vecs = extract_word_embeddings(en_hi_pairs, en_embeddings, hi_embeddings)\n",
        "\n",
        "        # Perform Procrustes alignment\n",
        "        W = orthogonal_procrustes(en_vecs, hi_vecs)\n",
        "\n",
        "        # Apply the learned mapping to all English word embeddings\n",
        "        aligned_en_embeddings = apply_mapping(en_embeddings, W)\n",
        "\n",
        "        # Perform translation\n",
        "        translations = translate_words(aligned_en_embeddings, hi_embeddings, top_k=5,limit_size=2000)\n",
        "\n",
        "        # Evaluate translation\n",
        "        test_dict = load_muse_test_dict('en-hi.txt')  # Using the same file for testing\n",
        "        precision_at_1, precision_at_5, accuracy = evaluate_translation(translations, test_dict)\n",
        "\n",
        "        results[size] = (precision_at_1, precision_at_5, accuracy)\n",
        "\n",
        "    return results\n",
        "\n",
        "ablation_results = perform_ablation_study(en_embeddings, hi_embeddings)\n",
        "\n",
        "for size, (p1, p5, acc) in ablation_results.items():\n",
        "    print(f\"Lexicon size: {size}\")\n",
        "    print(f\"  Precision@1: {p1:.4f}\")\n",
        "    print(f\"  Precision@5: {p5:.4f}\")\n",
        "    print(f\"  Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyiAdmlAEAME",
        "outputId": "2e5ed9b1-8bd4-485b-995b-2fb42e335773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing alignment with lexicon size: 5000\n",
            "Performing alignment with lexicon size: 10000\n",
            "Lexicon size: 5000\n",
            "  Precision@1: 0.4121\n",
            "  Precision@5: 0.7208\n",
            "  Accuracy: 0.7208\n",
            "Lexicon size: 10000\n",
            "  Precision@1: 0.3962\n",
            "  Precision@5: 0.7080\n",
            "  Accuracy: 0.7080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x4OTOUtFKlBT"
      }
    }
  ]
}